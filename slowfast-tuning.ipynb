{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-21T00:33:41.547638Z","iopub.execute_input":"2022-01-21T00:33:41.547912Z","iopub.status.idle":"2022-01-21T00:33:41.554928Z","shell.execute_reply.started":"2022-01-21T00:33:41.54788Z","shell.execute_reply":"2022-01-21T00:33:41.553745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_dir = \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo\"\ncheckpoint_paths = {\n    \"slowfast_r50\": f\"{root_dir}/kinetics/SLOWFAST_8x8_R50.pyth\",\n    \"slowfast_r50_detection\": f\"{root_dir}/ava/SLOWFAST_8x8_R50_DETECTION.pyth\",\n    \"slowfast_r101\": f\"{root_dir}/kinetics/SLOWFAST_8x8_R101.pyth\",\n    \"slowfast_16x8_r101_50_50\": f\"{root_dir}/kinetics/SLOWFAST_16x8_R101_50_50.pyth\",\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T00:33:41.556124Z","iopub.status.idle":"2022-01-21T00:33:41.557039Z","shell.execute_reply.started":"2022-01-21T00:33:41.556792Z","shell.execute_reply":"2022-01-21T00:33:41.556818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2022-01-21T00:33:41.558586Z","iopub.status.idle":"2022-01-21T00:33:41.559335Z","shell.execute_reply.started":"2022-01-21T00:33:41.558983Z","shell.execute_reply":"2022-01-21T00:33:41.559072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-01-24T16:51:30.738602Z","iopub.execute_input":"2022-01-24T16:51:30.738824Z","iopub.status.idle":"2022-01-24T16:51:31.429044Z","shell.execute_reply.started":"2022-01-24T16:51:30.738796Z","shell.execute_reply":"2022-01-24T16:51:31.428162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorchvideo","metadata":{"execution":{"iopub.status.busy":"2022-01-21T00:33:41.561053Z","iopub.status.idle":"2022-01-21T00:33:41.561759Z","shell.execute_reply.started":"2022-01-21T00:33:41.561472Z","shell.execute_reply":"2022-01-21T00:33:41.561498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"import argparse\nimport math\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom pytorchvideo.data import make_clip_sampler, labeled_video_dataset\nfrom pytorchvideo.models import create_slowfast\nfrom torch.backends import cudnn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n#from utils import train_transform, test_transform, clip_duration, num_classes\n\nfrom pytorchvideo.transforms import ApplyTransformToKey, UniformTemporalSubsample, RandomShortSideScale, \\\n    ShortSideScale, Normalize\nfrom torch import nn\nfrom torchvision.transforms import Compose, Lambda, RandomCrop, RandomHorizontalFlip, CenterCrop\n\nside_size = 256\nmax_size = 256\nmean = [0.45, 0.45, 0.45]\nstd = [0.225, 0.225, 0.225]\ncrop_size = 100\nnum_frames = 32\nsampling_rate = 1\nframes_per_second = 32/6\nclip_duration = (num_frames * sampling_rate) / frames_per_second\nnum_classes = 3\ncheckpoint_path = '/kaggle/working/SLOWFAST_8x8_R50.pyth'\n\ndata_root = \"/kaggle/input/prevention/all\"\nbatch_size = 6\nepochs = 50\nsave_root = '/kaggle/working/CheckPoints/Batch_2_sgd_lr001'\n\n# for reproducibility\nrandom.seed(1)\nnp.random.seed(1)\ntorch.manual_seed(1)\ncudnn.deterministic = True\ncudnn.benchmark = True\n\n\n\n\nclass PackPathway(nn.Module):\n    \"\"\"\n    Transform for converting video frames as a list of tensors.\n    \"\"\"\n\n    def __init__(self, alpha=4):\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, frames):\n        fast_pathway = frames\n        # perform temporal sampling from the fast pathway.\n        slow_pathway = torch.index_select(frames, 1,\n                                          torch.linspace(0, frames.shape[1] - 1, frames.shape[1] // self.alpha).long())\n        frame_list = [slow_pathway, fast_pathway]\n        return frame_list\n\n\ntrain_transform = ApplyTransformToKey(key=\"video\", transform=Compose(\n    [UniformTemporalSubsample(num_frames), Lambda(lambda x: x / 255.0), Normalize(mean, std), ShortSideScale(size=side_size), PackPathway()]))\n\ntest_transform = ApplyTransformToKey(key=\"video\", transform=Compose(\n    [UniformTemporalSubsample(num_frames), Lambda(lambda x: x / 255.0), Normalize(mean, std), ShortSideScale(size=side_size), PackPathway()]))\n\n\n\n# train for one epoch\ndef train(model, data_loader, train_optimizer):\n    model.train()\n    total_loss, total_acc, total_num = 0.0, 0, 0\n    train_bar = tqdm(data_loader, total=math.ceil(train_data.num_videos / batch_size), dynamic_ncols=True)\n    for batch in train_bar:\n        video, label = [i.cuda() for i in batch['video']], batch['label'].cuda()\n        \n        train_optimizer.zero_grad()\n        pred = model(video)\n        loss = loss_criterion(pred, label)\n        total_loss += loss.item() * video[0].size(0)\n        total_acc += (torch.eq(pred.argmax(dim=-1), label)).sum().item()\n        loss.backward()\n        train_optimizer.step()\n\n        total_num += video[0].size(0)\n        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f} Acc: {:.2f}%'\n                                  .format(epoch, epochs, total_loss / total_num, total_acc * 100 / total_num))\n\n    return total_loss / total_num, total_acc / total_num\n\n\n# test for one epoch\ndef val(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        total_top_1, total_top_5, total_num = 0, 0, 0\n        test_bar = tqdm(data_loader, total=math.ceil(test_data.num_videos / batch_size), dynamic_ncols=True)\n        for batch in test_bar:\n            video, label = [i.cuda() for i in batch['video']], batch['label'].cuda()\n            pred = model(video)\n            total_top_1 += (torch.eq(pred.argmax(dim=-1), label)).sum().item()\n            total_top_5 += torch.any(torch.eq(pred.topk(k=2, dim=-1).indices, label.unsqueeze(dim=-1)),\n                                     dim=-1).sum().item()\n            total_num += video[0].size(0)\n            test_bar.set_description('Test Epoch: [{}/{}] | Top-1:{:.2f}% | Top-5:{:.2f}%'\n                                     .format(epoch, epochs, total_top_1 * 100 / total_num,\n                                             total_top_5 * 100 / total_num))\n    return total_top_1 / total_num, total_top_5 / total_num\n\n'''\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Train Model')\n    # common args\n    parser.add_argument('--data_root', default='data', type=str, help='Datasets root path')\n    parser.add_argument('--batch_size', default=8, type=int, help='Number of videos in each mini-batch')\n    parser.add_argument('--epochs', default=10, type=int, help='Number of epochs over the model to train')\n    parser.add_argument('--save_root', default='result', type=str, help='Result saved root path')\n    \n# args parse\nargs = parser.parse_args()\n'''\n\n\n# data prepare\ntrain_data = labeled_video_dataset('{}/train'.format(data_root), make_clip_sampler('random', clip_duration),\n                                   transform=train_transform, decode_audio=False)\ntest_data = labeled_video_dataset('{}/test'.format(data_root),\n                                  make_clip_sampler('constant_clips_per_video', clip_duration, 1),\n                                  transform=test_transform, decode_audio=False)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, num_workers=8)\ntest_loader = DataLoader(test_data, batch_size=batch_size, num_workers=8)\n\n\n#------------------------------------------------------------------------------------------------------------\n\n# model define, loss setup and optimizer config\n#slow_fast = create_slowfast(model_num_class=num_classes).cuda()\n\n\nslow_fast = torch.hub.load('facebookresearch/pytorchvideo:main', model='slowfast_r50', pretrained=True).cuda()\n\n\n#------------------------------------------------------------------------------------------------------------\n\n\nloss_criterion = CrossEntropyLoss()\n# optimizer = Adam(slow_fast.parameters(), lr=1e-1)\noptimizer = SGD(slow_fast.parameters(), lr=0.001, momentum=0.9)\n\n# training loop\nresults = {'loss': [], 'acc': [], 'top-1': [], 'top-5': []}\nif not os.path.exists(save_root):\n    os.makedirs(save_root)\nbest_acc = 0.0\nfor epoch in range(1, epochs + 1):\n    train_loss, train_acc = train(slow_fast, train_loader, optimizer)\n    results['loss'].append(train_loss)\n    results['acc'].append(train_acc * 100)\n    top_1, top_5 = val(slow_fast, test_loader)\n    results['top-1'].append(top_1 * 100)\n    results['top-5'].append(top_5 * 100)\n    # save statistics\n    data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n    data_frame.to_csv('{}/metrics.csv'.format(save_root), index_label='epoch')\n\n    if top_1 > best_acc:\n        best_acc = top_1\n        torch.save(slow_fast.state_dict(), '{}/slow_fast.pth'.format(save_root))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T00:33:41.630969Z","iopub.execute_input":"2022-01-21T00:33:41.631415Z","iopub.status.idle":"2022-01-21T00:33:53.935651Z","shell.execute_reply.started":"2022-01-21T00:33:41.631361Z","shell.execute_reply":"2022-01-21T00:33:53.932324Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 继续训练","metadata":{}},{"cell_type":"code","source":"!pip install pytorchvideo","metadata":{"execution":{"iopub.status.busy":"2022-01-26T18:19:14.587519Z","iopub.execute_input":"2022-01-26T18:19:14.589263Z","iopub.status.idle":"2022-01-26T18:19:35.405824Z","shell.execute_reply.started":"2022-01-26T18:19:14.589108Z","shell.execute_reply":"2022-01-26T18:19:35.404552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport math\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom pytorchvideo.data import make_clip_sampler, labeled_video_dataset\nfrom pytorchvideo.models import create_slowfast\nfrom torch.backends import cudnn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n#from utils import train_transform, test_transform, clip_duration, num_classes\n\nfrom pytorchvideo.transforms import ApplyTransformToKey, UniformTemporalSubsample, RandomShortSideScale, \\\n    ShortSideScale, Normalize\nfrom torch import nn\nfrom torchvision.transforms import Compose, Lambda, RandomCrop, RandomHorizontalFlip, CenterCrop\n\nside_size = 256\nmax_size = 256\nmean = [0.45, 0.45, 0.45]\nstd = [0.225, 0.225, 0.225]\ncrop_size = 100\nnum_frames = 32\nsampling_rate = 1\nframes_per_second = 32/6\nclip_duration = (num_frames * sampling_rate) / frames_per_second\nnum_classes = 3\ncheckpoint_path = '/kaggle/working/SLOWFAST_8x8_R50.pyth'\n\ndata_root = \"/kaggle/input/prevention/all\"\nbatch_size = 6\nepochs = 50\nsave_root = '/kaggle/working/CheckPoints/Batch_2_sgd_lr001'\n\n# for reproducibility\nrandom.seed(1)\nnp.random.seed(1)\ntorch.manual_seed(1)\ncudnn.deterministic = True\ncudnn.benchmark = True\n\n\n\n\nclass PackPathway(nn.Module):\n    \"\"\"\n    Transform for converting video frames as a list of tensors.\n    \"\"\"\n\n    def __init__(self, alpha=4):\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, frames):\n        fast_pathway = frames\n        # perform temporal sampling from the fast pathway.\n        slow_pathway = torch.index_select(frames, 1,\n                                          torch.linspace(0, frames.shape[1] - 1, frames.shape[1] // self.alpha).long())\n        frame_list = [slow_pathway, fast_pathway]\n        return frame_list\n\n\ntrain_transform = ApplyTransformToKey(key=\"video\", transform=Compose(\n    [UniformTemporalSubsample(num_frames), Lambda(lambda x: x / 255.0), Normalize(mean, std), ShortSideScale(size=side_size), PackPathway()]))\n\ntest_transform = ApplyTransformToKey(key=\"video\", transform=Compose(\n    [UniformTemporalSubsample(num_frames), Lambda(lambda x: x / 255.0), Normalize(mean, std), ShortSideScale(size=side_size), PackPathway()]))\n\n\n\n# train for one epoch\ndef train(model, data_loader, train_optimizer):\n    model.train()\n    total_loss, total_acc, total_num = 0.0, 0, 0\n    train_bar = tqdm(data_loader, total=math.ceil(train_data.num_videos / batch_size), dynamic_ncols=True)\n    for batch in train_bar:\n        video, label = [i.cuda() for i in batch['video']], batch['label'].cuda()\n        \n        train_optimizer.zero_grad()\n        pred = model(video)\n        loss = loss_criterion(pred, label)\n        total_loss += loss.item() * video[0].size(0)\n        total_acc += (torch.eq(pred.argmax(dim=-1), label)).sum().item()\n        loss.backward()\n        train_optimizer.step()\n\n        total_num += video[0].size(0)\n        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f} Acc: {:.2f}%'\n                                  .format(epoch, epochs, total_loss / total_num, total_acc * 100 / total_num))\n\n    return total_loss / total_num, total_acc / total_num\n\n\n# test for one epoch\ndef val(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        total_top_1, total_top_5, total_num = 0, 0, 0\n        test_bar = tqdm(data_loader, total=math.ceil(test_data.num_videos / batch_size), dynamic_ncols=True)\n        for batch in test_bar:\n            video, label = [i.cuda() for i in batch['video']], batch['label'].cuda()\n            pred = model(video)\n            total_top_1 += (torch.eq(pred.argmax(dim=-1), label)).sum().item()\n            total_top_5 += torch.any(torch.eq(pred.topk(k=2, dim=-1).indices, label.unsqueeze(dim=-1)),\n                                     dim=-1).sum().item()\n            total_num += video[0].size(0)\n            test_bar.set_description('Test Epoch: [{}/{}] | Top-1:{:.2f}% | Top-5:{:.2f}%'\n                                     .format(epoch, epochs, total_top_1 * 100 / total_num,\n                                             total_top_5 * 100 / total_num))\n    return total_top_1 / total_num, total_top_5 / total_num\n\n'''\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Train Model')\n    # common args\n    parser.add_argument('--data_root', default='data', type=str, help='Datasets root path')\n    parser.add_argument('--batch_size', default=8, type=int, help='Number of videos in each mini-batch')\n    parser.add_argument('--epochs', default=10, type=int, help='Number of epochs over the model to train')\n    parser.add_argument('--save_root', default='result', type=str, help='Result saved root path')\n    parser.add_argument(\"--learning_rate\", default=3e-2, type=float,help=\"The initial learning rate for SGD.\")\n    parser.add_argument(\"--weight_decay\", default=0, type=float,help=\"Weight deay if we apply some.\")\n# args parse\nargs = parser.parse_args()\n'''\n\n\n# data prepare\ntrain_data = labeled_video_dataset('{}/train'.format(data_root), make_clip_sampler('random', clip_duration),\n                                   transform=train_transform, decode_audio=False)\ntest_data = labeled_video_dataset('{}/test'.format(data_root),\n                                  make_clip_sampler('constant_clips_per_video', clip_duration, 1),\n                                  transform=test_transform, decode_audio=False)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, num_workers=8)\ntest_loader = DataLoader(test_data, batch_size=batch_size, num_workers=8)\n\n\n#------------------------------------------------------------------------------------------------------------\n\n# model define, loss setup and optimizer config\n#slow_fast = create_slowfast(model_num_class=num_classes).cuda()\n\n\nslow_fast = torch.hub.load('facebookresearch/pytorchvideo:main', model='slowfast_r50', pretrained=True).cuda()\n\n#slow_fast.load_state_dict(torch.load('/kaggle/working/CheckPoints/Batch_2_sgd_lr001/slow_fast.pth', 'cuda'))\nslow_fast.blocks[6].proj = torch.nn.Linear(in_features=2304, out_features=3, bias=True).cuda()\n\n#------------------------------------------------------------------------------------------------------------\n\n\nloss_criterion = CrossEntropyLoss()\n# optimizer = Adam(slow_fast.parameters(), lr=1e-1)\n#optimizer = SGD(slow_fast.parameters(), lr=0.001, momentum=0.9,weight_decay=0.0001)\n\n\n# optimizer = SGD([{'params':slow_fast.parameters(),'lr':args.learning_rate},{'params':model.head.parameters(),'lr':args.learning_rate}],\n#                 lr=args.learning_rate,momentum=0.9,weight_decay=args.weight_decay)\n\noptimizer = SGD([{'params':slow_fast.blocks[0:6].parameters(),'lr':0.0001},\n                 {'params':slow_fast.blocks[6].dropout.parameters(),'lr':0.0001},\n                 {'params':slow_fast.blocks[6].proj.parameters(),'lr':0.001},\n                 {'params':slow_fast.blocks[6].output_pool.parameters(),'lr':0.0001}], \n                lr=0.0001,momentum=0.9,weight_decay=0.0001)\n\n\n\nprint(optimizer)\n\n##---------------------------------------------------------------------------------------------------------\n# training loop\nresults = {'loss': [], 'acc': [], 'top-1': [], 'top-5': []}\nif not os.path.exists(save_root):\n    os.makedirs(save_root)\nbest_acc = 0.0\nfor epoch in range(1, epochs + 1):\n    train_loss, train_acc = train(slow_fast, train_loader, optimizer)\n    results['loss'].append(train_loss)\n    results['acc'].append(train_acc * 100)\n    top_1, top_5 = val(slow_fast, test_loader)\n    results['top-1'].append(top_1 * 100)\n    results['top-5'].append(top_5 * 100)\n    # save statistics\n    data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n    data_frame.to_csv('{}/metrics.csv'.format(save_root), index_label='epoch')\n\n    if top_1 > best_acc:\n        best_acc = top_1\n        torch.save(slow_fast.state_dict(), '{}/slow_fast.pth'.format(save_root))","metadata":{"execution":{"iopub.status.busy":"2022-01-26T18:51:06.579467Z","iopub.execute_input":"2022-01-26T18:51:06.579763Z","iopub.status.idle":"2022-01-26T20:49:26.518961Z","shell.execute_reply.started":"2022-01-26T18:51:06.579731Z","shell.execute_reply":"2022-01-26T20:49:26.515624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer","metadata":{"execution":{"iopub.status.busy":"2022-01-26T18:36:19.970399Z","iopub.execute_input":"2022-01-26T18:36:19.970688Z","iopub.status.idle":"2022-01-26T18:36:19.982856Z","shell.execute_reply.started":"2022-01-26T18:36:19.970656Z","shell.execute_reply":"2022-01-26T18:36:19.980683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slow_fast.blocks[6]","metadata":{"execution":{"iopub.status.busy":"2022-01-26T18:46:10.962306Z","iopub.execute_input":"2022-01-26T18:46:10.962613Z","iopub.status.idle":"2022-01-26T18:46:10.969291Z","shell.execute_reply.started":"2022-01-26T18:46:10.96258Z","shell.execute_reply":"2022-01-26T18:46:10.968315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slow_fast.blocks[6].proj = torch.nn.Linear(in_features=2304, out_features=3, bias=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T18:16:12.989311Z","iopub.execute_input":"2022-01-26T18:16:12.989639Z","iopub.status.idle":"2022-01-26T18:16:12.996618Z","shell.execute_reply.started":"2022-01-26T18:16:12.989595Z","shell.execute_reply":"2022-01-26T18:16:12.995576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train 1\n\n224*224\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n  \n  \nTrain Epoch: [1/50] Loss: 1.1127  **Acc: 41.06%**: : 1312it [34:25,  1.57s/it]                        \nTest Epoch: [1/50] |  **Top-1:39.55%**  | Top-5:74.07%: 100%|██████████| 560/560 [13:27<00:00,  1.44s/it]\nTrain Epoch: [2/50] Loss: 1.0590  **Acc: 46.50%**: : 1312it [34:30,  1.58s/it]                        \nTest Epoch: [2/50] |  **Top-1:40.89%**  | Top-5:71.98%: 100%|██████████| 560/560 [13:35<00:00,  1.46s/it]\nTrain Epoch: [3/50] Loss: 1.0099  **Acc: 49.37%:** : 1312it [34:23,  1.57s/it]                        \nTest Epoch: [3/50] |  **Top-1:43.43%**  | Top-5:71.56%: 100%|██████████| 560/560 [13:38<00:00,  1.46s/it]\nTrain Epoch: [4/50] Loss: 0.9751  **Acc: 51.94%**  : : 1312it [34:44,  1.59s/it]                        \nTest Epoch: [4/50] |  **Top-1:42.83%**  | Top-5:71.59%: 100%|██████████| 560/560 [13:15<00:00,  1.42s/it]\nTrain Epoch: [5/50] Loss: 0.9386  ** Acc: 54.57%**  : : 1312it [33:56,  1.55s/it]                        \nTest Epoch: [5/50] |  **Top-1:44.05%**  | Top-5:74.22%: 100%|██████████| 560/560 [13:32<00:00,  1.45s/it]\nTrain Epoch: [6/50] Loss: 0.9006  **Acc: 57.53%**  : : 1312it [33:42,  1.54s/it]                        \nTest Epoch: [6/50] |  **Top-1:35.74%**  | Top-5:73.23%: 100%|██████████| 560/560 [13:43<00:00,  1.47s/it]\nTrain Epoch: [7/50] Loss: 0.8706  **Acc: 58.79%**  : : 1312it [34:28,  1.58s/it]                        \nTest Epoch: [7/50] |  **Top-1:41.04%**  | Top-5:72.10%: 100%|██████████| 560/560 [13:56<00:00,  1.49s/it] \nTrain Epoch: [8/50] Loss: 0.8357  **Acc: 61.38%**  : : 1312it [34:41,  1.59s/it]                        \nTest Epoch: [8/50] |  **Top-1:39.49%**  | Top-5:73.35%: 100%|██████████| 560/560 [13:49<00:00,  1.48s/it]\nTrain Epoch: [9/50] Loss: 0.7987  **Acc: 62.91%**  : : 1312it [34:04,  1.56s/it]                        \nTest Epoch: [9/50] |  **Top-1:42.56%**  | Top-5:70.64%: 100%|██████████| 560/560 [13:33<00:00,  1.45s/it] \nTrain Epoch: [10/50] Loss: 0.7658 **Acc: 66.39%**:   : 1312it [34:24,  1.57s/it]                        \nTest Epoch: [10/50] | **Top-1:41.13%**  | Top-5:75.02%: 100%|██████████| 560/560 [13:27<00:00,  1.44s/it]\nTrain Epoch: [11/50] Loss: 0.7220 **Acc: 67.77%**  : : 1312it [34:26,  1.58s/it]                        \nTest Epoch: [11/50] | **Top-1:35.89%**  | Top-5:74.22%: 100%|██████████| 560/560 [13:47<00:00,  1.48s/it] \nTrain Epoch: [12/50] Loss: 0.7307 **Acc: 68.45%**  :  32%|███▏      | 421/1308 [11:22<23:50,  1.61s/it]\n\n## train\n***with pretrained weight***  - 224*224\n\n\nTrain Epoch: [1/50] Loss: 0.5032 Acc: 81.64%: : 1312it [34:36,  1.58s/it]                        \nTest Epoch: [1/50] | Top-1:45.01% | Top-5:74.93%: 100%|██████████| 560/560 [13:47<00:00,  1.48s/it]\nTrain Epoch: [2/50] Loss: 0.1541 Acc: 94.38%:   7%|▋         | 92/1308 [02:40<31:32,  1.56s/it]  \n","metadata":{}},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"import argparse\nimport json\n\nimport torch\nfrom pytorchvideo.data.encoded_video import EncodedVideo\nfrom pytorchvideo.models import create_slowfast\n\nfrom utils import num_classes, clip_duration, test_transform\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Test Model')\n    parser.add_argument('--model_path', default='result/slow_fast.pth', type=str, help='Model path')\n    parser.add_argument('--video_path', default='data/test/applauding/_V-dzjftmCQ_000023_000033.mp4', type=str,\n                        help='Video path')\n\n    opt = parser.parse_args()\n    model_path, video_path = opt.model_path, opt.video_path\n    slow_fast = create_slowfast(model_num_class=num_classes)\n    slow_fast.load_state_dict(torch.load(model_path, 'cpu'))\n    slow_fast = slow_fast.cuda().eval()\n    with open('result/kinetics_classnames.json', 'r') as f:\n        kinetics_classnames = json.load(f)\n\n    # create an id to label name mapping\n    kinetics_id_to_classname = {}\n    for k, v in kinetics_classnames.items():\n        kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\n\n    video = EncodedVideo.from_path(video_path, decode_audio=False)\n    video_data = video.get_clip(start_sec=0, end_sec=clip_duration)\n    video_data = test_transform(video_data)\n    inputs = [i.cuda()[None, ...] for i in video_data['video']]\n    pred = slow_fast(inputs)\n\n    # get the predicted classes\n    pred_classes = pred.topk(k=5).indices\n    pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes[0]]\n    print('predicted labels: {}'.format(pred_class_names))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T00:33:53.941767Z","iopub.status.idle":"2022-01-21T00:33:53.947503Z","shell.execute_reply.started":"2022-01-21T00:33:53.947216Z","shell.execute_reply":"2022-01-21T00:33:53.947251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Metrics","metadata":{}},{"cell_type":"code","source":"!pip install pytorchvideo","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport math\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom pytorchvideo.data import make_clip_sampler, labeled_video_dataset\nfrom pytorchvideo.models import create_slowfast\nfrom torch.backends import cudnn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n#from utils import train_transform, test_transform, clip_duration, num_classes\n\nfrom pytorchvideo.transforms import ApplyTransformToKey, UniformTemporalSubsample, RandomShortSideScale, \\\n    ShortSideScale, Normalize\nfrom torch import nn\nfrom torchvision.transforms import Compose, Lambda, RandomCrop, RandomHorizontalFlip, CenterCrop\n\nside_size = 256\nmax_size = 256\nmean = [0.45, 0.45, 0.45]\nstd = [0.225, 0.225, 0.225]\ncrop_size = 100\nnum_frames = 32\nsampling_rate = 1\nframes_per_second = 32/6\nclip_duration = (num_frames * sampling_rate) / frames_per_second\nnum_classes = 3\ncheckpoint_path = '/kaggle/working/SLOWFAST_8x8_R50.pyth'\n\ndata_root = \"/kaggle/input/prevention/all\"\nbatch_size = 6\nepochs = 1\nsave_root = '/kaggle/working/CheckPoints/Batch_2_sgd_lr001'\n\n# for reproducibility\nrandom.seed(1)\nnp.random.seed(1)\ntorch.manual_seed(1)\ncudnn.deterministic = True\ncudnn.benchmark = True\n\n\n\n\nclass PackPathway(nn.Module):\n    \"\"\"\n    Transform for converting video frames as a list of tensors.\n    \"\"\"\n\n    def __init__(self, alpha=4):\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, frames):\n        fast_pathway = frames\n        # perform temporal sampling from the fast pathway.\n        slow_pathway = torch.index_select(frames, 1,\n                                          torch.linspace(0, frames.shape[1] - 1, frames.shape[1] // self.alpha).long())\n        frame_list = [slow_pathway, fast_pathway]\n        return frame_list\n\n\ntrain_transform = ApplyTransformToKey(key=\"video\", transform=Compose(\n    [UniformTemporalSubsample(num_frames), Lambda(lambda x: x / 255.0), Normalize(mean, std), ShortSideScale(size=side_size), PackPathway()]))\n\ntest_transform = ApplyTransformToKey(key=\"video\", transform=Compose(\n    [UniformTemporalSubsample(num_frames), Lambda(lambda x: x / 255.0), Normalize(mean, std), ShortSideScale(size=side_size), PackPathway()]))\n\n\n\n# train for one epoch\ndef train(model, data_loader, train_optimizer):\n    model.train()\n    total_loss, total_acc, total_num = 0.0, 0, 0\n    train_bar = tqdm(data_loader, total=math.ceil(train_data.num_videos / batch_size), dynamic_ncols=True)\n    for batch in train_bar:\n        video, label = [i.cuda() for i in batch['video']], batch['label'].cuda()\n        \n        train_optimizer.zero_grad()\n        pred = model(video)\n        loss = loss_criterion(pred, label)\n        total_loss += loss.item() * video[0].size(0)\n        total_acc += (torch.eq(pred.argmax(dim=-1), label)).sum().item()\n        loss.backward()\n        train_optimizer.step()\n\n        total_num += video[0].size(0)\n        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f} Acc: {:.2f}%'\n                                  .format(epoch, epochs, total_loss / total_num, total_acc * 100 / total_num))\n\n    return total_loss / total_num, total_acc / total_num\n\n\n# test for one epoch\ndef val(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        total_top_1, total_top_5, total_num = 0, 0, 0\n        test_bar = tqdm(data_loader, total=math.ceil(test_data.num_videos / batch_size), dynamic_ncols=True)\n        for batch in test_bar:\n            video, label = [i.cuda() for i in batch['video']], batch['label'].cuda()\n            \n            pred = model(video)\n            \n            #print(pred.argmax(dim=-1).cpu())\n            pred_result.extend(pred.argmax(dim=-1).cpu())\n            ground_truth.extend(label.cpu())\n            #print(label)\n\n            \n            total_top_1 += (torch.eq(pred.argmax(dim=-1), label)).sum().item()\n            total_top_5 += torch.any(torch.eq(pred.topk(k=2, dim=-1).indices, label.unsqueeze(dim=-1)),\n                                     dim=-1).sum().item()\n            total_num += video[0].size(0)\n            test_bar.set_description('Test Epoch: [{}/{}] | Top-1:{:.2f}% | Top-5:{:.2f}%'\n                                     .format(epoch, epochs, total_top_1 * 100 / total_num,\n                                             total_top_5 * 100 / total_num))\n    return total_top_1 / total_num, total_top_5 / total_num\n\n'''\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Train Model')\n    # common args\n    parser.add_argument('--data_root', default='data', type=str, help='Datasets root path')\n    parser.add_argument('--batch_size', default=8, type=int, help='Number of videos in each mini-batch')\n    parser.add_argument('--epochs', default=10, type=int, help='Number of epochs over the model to train')\n    parser.add_argument('--save_root', default='result', type=str, help='Result saved root path')\n    \n# args parse\nargs = parser.parse_args()\n'''\n\n\n# data prepare\ntrain_data = labeled_video_dataset('{}/train'.format(data_root), make_clip_sampler('random', clip_duration),\n                                   transform=train_transform, decode_audio=False)\ntest_data = labeled_video_dataset('{}/test'.format(data_root),\n                                  make_clip_sampler('constant_clips_per_video', clip_duration, 1),\n                                  transform=test_transform, decode_audio=False)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, num_workers=8)\ntest_loader = DataLoader(test_data, batch_size=batch_size, num_workers=8)\n\n\n#------------------------------------------------------------------------------------------------------------\n\n# model define, loss setup and optimizer config\n#slow_fast = create_slowfast(model_num_class=num_classes).cuda()\n\n\nslow_fast = torch.hub.load('facebookresearch/pytorchvideo:main', model='slowfast_r50', pretrained=True).cuda()\n\nslow_fast.load_state_dict(torch.load('/kaggle/input/pretrained-weight/slow_fast.pth', 'cuda'))\n\n#------------------------------------------------------------------------------------------------------------\n\n\nloss_criterion = CrossEntropyLoss()\n# optimizer = Adam(slow_fast.parameters(), lr=1e-1)\noptimizer = SGD(slow_fast.parameters(), lr=0.001, momentum=0.9)\n\n# training loop\nresults = {'loss': [], 'acc': [], 'top-1': [], 'top-5': []}\n\npred_result = []\nground_truth = []\n\nif not os.path.exists(save_root):\n    os.makedirs(save_root)\nbest_acc = 0.0\nfor epoch in range(1, epochs + 1):\n    train_loss, train_acc = train(slow_fast, train_loader, optimizer)\n    results['loss'].append(train_loss)\n    results['acc'].append(train_acc * 100)\n    top_1, top_5 = val(slow_fast, test_loader)\n    results['top-1'].append(top_1 * 100)\n    results['top-5'].append(top_5 * 100)\n    \n    #pred_result.extend(pred)\n    #ground_truth.extend(label)\n    \n    \n    # save statistics\n    data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n    data_frame.to_csv('{}/metrics.csv'.format(save_root), index_label='epoch')\n\n    if top_1 > best_acc:\n        best_acc = top_1\n        torch.save(slow_fast.state_dict(), '{}/slow_fast.pth'.format(save_root))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:32:51.07149Z","iopub.execute_input":"2022-01-21T17:32:51.071966Z","iopub.status.idle":"2022-01-21T17:43:34.672339Z","shell.execute_reply.started":"2022-01-21T17:32:51.071925Z","shell.execute_reply":"2022-01-21T17:43:34.671214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(ground_truth), len(pred_result)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:50:43.995574Z","iopub.execute_input":"2022-01-21T17:50:43.995835Z","iopub.status.idle":"2022-01-21T17:50:44.003848Z","shell.execute_reply.started":"2022-01-21T17:50:43.995806Z","shell.execute_reply":"2022-01-21T17:50:44.003121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ground_truth[i for i in range(len(ground_truth))].numpy()\n\n\na = [i.numpy() for i in ground_truth]\nb = [i.numpy() for i in pred_result]\n\n#a.numpy()\n#ground_truth[i for i in range(3355)].numpy()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T17:56:23.664803Z","iopub.execute_input":"2022-01-21T17:56:23.665056Z","iopub.status.idle":"2022-01-21T17:56:23.673441Z","shell.execute_reply.started":"2022-01-21T17:56:23.665027Z","shell.execute_reply":"2022-01-21T17:56:23.672741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport pandas as pd\n\nclasses = ('non', 'left', 'right')\n\n# Build confusion matrix\ncf_matrix = confusion_matrix(a, b)\ndf_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) * 3, index = [i for i in classes],\n                     columns = [i for i in classes])\nplt.figure(figsize = (3,3))\nsn.heatmap(df_cm, annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T18:05:02.604888Z","iopub.execute_input":"2022-01-21T18:05:02.605276Z","iopub.status.idle":"2022-01-21T18:05:02.88461Z","shell.execute_reply.started":"2022-01-21T18:05:02.605225Z","shell.execute_reply":"2022-01-21T18:05:02.883905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"/home/k/kai/checkpoint\n/home/k/kai/data/all\n\nslowfast_diff_lr.py","metadata":{},"execution_count":null,"outputs":[]}]}